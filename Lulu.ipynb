{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOr0hJCx28iGFtP91J2i3wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prince-musonda/Lulu/blob/main/Lulu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# set up, and install any necessary packages\n",
        "\n"
      ],
      "metadata": {
        "id": "k9rOn5GGrkIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kokoro>=0.9.2 soundfile flask\n",
        "!apt-get -qq -y install espeak-ng > /dev/null 2>&1\n",
        "!pip install -q flask pyngrok\n"
      ],
      "metadata": {
        "id": "7nB0yWfarDdt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3bMk-naA-R10"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "from kokoro import KPipeline\n",
        "import soundfile as sf\n",
        "from flask import Flask, request, send_file\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up text to speech"
      ],
      "metadata": {
        "id": "q7Gk54VfuAeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_speech(text:str,path:str):\n",
        "    \"\"\"\n",
        "    text: input string to convert to audio /speech\n",
        "    path: path/filename dot file extention to save the file\n",
        "    \"\"\"\n",
        "    speech_generator = KPipeline(lang_code='a')\n",
        "    generated_speech = speech_generator(text, voice='af_heart')\n",
        "    for _,_,audio in generated_speech:\n",
        "      #save generated speech into audio file\n",
        "      sf.write(path, audio,samplerate=23000)"
      ],
      "metadata": {
        "id": "nJYUIbvauNda"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and model Preprocessor  setup"
      ],
      "metadata": {
        "id": "tpO6cgTIuUnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
        "preprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\",dtype=torch.float16, device_map='auto')"
      ],
      "metadata": {
        "id": "58MNiTOT_DE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "configuration_1 = [\n",
        "    {\n",
        "        \"role\": 'user',\n",
        "        'content': [\n",
        "            {\"type\":\"text\", \"text\": \"\"\"I am a blind person.\n",
        "            Tell me what you see in a way that is helpful for me. if there is\n",
        "            any traffic lights tell me what the status is, including whether it's safe to cross the streets or not,\n",
        "            whether  near a staircase, and warn about Obstacles.\n",
        "            And start the sentence with the phrase: \"I see\"\n",
        "            \"\"\"},\n",
        "\n",
        "            {\"type\": \"image\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "def model_inference(image:Image):\n",
        "  # unified prompt that combines the text with the image\n",
        "  prompt = preprocessor.apply_chat_template(configuration_1, add_generation_prompt=True)\n",
        "  input_data = preprocessor(images = image, text=prompt, return_tensors = 'pt').to(device='cuda',dtype=torch.float16)\n",
        "  output_token = model.generate(**input_data,max_new_tokens = 300)\n",
        "  output_str = preprocessor.decode(output_token[0],skip_special_tokens = True)\n",
        "  # clean up output by turn it a list and get the second part of the element list because that\n",
        "  # corresponds to output that we are interested in\n",
        "  output_str = output_str.split(\"ASSISTANT: \")\n",
        "  output_str = output_str[1]\n",
        "  return output_str\n"
      ],
      "metadata": {
        "id": "tFE3mqqd_3Ga"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Server set up"
      ],
      "metadata": {
        "id": "2BUX1ilh39AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "    # you can uncomment the 2 lines below  and comment the 4th line if you want wan't to use the original url\n",
        "# ngrok.set_auth_token('36AnchlKkoIZOiSpmc0SfqQa0ac_VRaAXWA1VH2xA6YKdfQx')\n",
        "# tunnel = ngrok.connect(5000,bind_tls=True,url='mai-unvisceral-finickily.ngrok-free.dev')\n",
        "    # comment this line below if you choose to uncomment the 2 lines above to use a randomly assigned url\n",
        "tunnel = ngrok.connect(5000,bind_tls=True)\n",
        "\n",
        "\n",
        "@app.route(\"/send_data\",methods=[\"POST\"])\n",
        "def upload_file():\n",
        "    if request.files:\n",
        "        user_uploaded_file = request.files.get('file')\n",
        "        #open upload file as image using the PILLow image class\n",
        "        image = Image.open(user_uploaded_file)\n",
        "        # perform inference of the image\n",
        "        predicted_description = model_inference(image)\n",
        "        # convert text description into speech\n",
        "        create_speech(predicted_description,'/content/audio_description.wav')\n",
        "        # send audio file\n",
        "        return send_file('/content/audio_description.wav', as_attachment=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print(\"please, connet to this address in your Lulu  device: \"+tunnel.public_url)\n",
        "  app.run(port=5000, use_reloader=False)\n"
      ],
      "metadata": {
        "id": "KVjIN1sML2Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jOvncYNJqzNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}